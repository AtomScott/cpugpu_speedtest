{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import report, training, Chain, datasets, iterators, optimizers\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import tuple_dataset\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # Chainerがそれぞれの層を大きさを推測する\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mnist(    \n",
    "    batchsize = 100,\n",
    "    epoch = 50,\n",
    "    gpu0 = -1,\n",
    "    gpu1 = None,\n",
    "    out = 'result',\n",
    "    resume = '',\n",
    "    unit = 1000,\n",
    "    ):\n",
    "    \n",
    "    print('GPU: {}, {}'.format(gpu0, gpu1))\n",
    "#     print('# unit: {}'.format(unit))\n",
    "#     print('# Minibatch-size: {}'.format(batchsize))\n",
    "#     print('# epoch: {}'.format(epoch))\n",
    "#     print('')\n",
    "\n",
    "    # Set up a neural network to train\n",
    "    # Classifier reports softmax cross entropy loss and accuracy at every\n",
    "    # iteration, which will be used by the PrintReport extension below.\n",
    "    model = L.Classifier(MLP(unit, 10))\n",
    "    if gpu0 >= 0:\n",
    "        # Make a specified GPU current\n",
    "        chainer.backends.cuda.get_device_from_id(gpu0).use()\n",
    "        if gpu1 is None:\n",
    "            model.to_gpu()  # Copy the model to the GPU\n",
    "\n",
    "    # Setup an optimizer\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    # Load the MNIST dataset\n",
    "    train, test = chainer.datasets.get_mnist()\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "\n",
    "    # Set up a trainer\n",
    "    # ParallelUpdater implements the data-parallel gradient computation on\n",
    "    # multiple GPUs. It accepts \"devices\" argument that specifies which GPU to\n",
    "    # use.\n",
    "    if gpu1 is None:\n",
    "        updater = training.updaters.StandardUpdater(\n",
    "            train_iter, optimizer, device=gpu0)\n",
    "    if gpu1 is not None:\n",
    "        updater = training.updaters.ParallelUpdater(\n",
    "            train_iter,\n",
    "            optimizer,\n",
    "            # The device of the name 'main' is used as a \"master\", while others are\n",
    "            # used as slaves. Names other than 'main' are arbitrary.\n",
    "            devices={'main': gpu0, 'second': gpu1},\n",
    "        )\n",
    "\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu0),trigger=(epoch, 'epoch'))\n",
    "\n",
    "    trainer.extend(extensions.LogReport(),trigger=(epoch, 'epoch'))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),trigger=(epoch, 'epoch'))\n",
    "    print('\\n')\n",
    "    # Run the training\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1, None\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          1.4019e-06  0.160681              1              0.9843                    370.94        \n",
      "GPU: 0, None\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          1.28746e-07  0.169154              1              0.9817                    86.7451       \n",
      "GPU: 0, 1\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          0.00607274  0.187425              1              0.9822                    185.336       \n"
     ]
    }
   ],
   "source": [
    "run_mnist()\n",
    "run_mnist(gpu0=0)\n",
    "run_mnist(gpu0=0, gpu1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Chain):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(\n",
    "                in_channels=None, out_channels=6, ksize=5, stride=1)\n",
    "            self.conv2 = L.Convolution2D(\n",
    "                in_channels=6, out_channels=16, ksize=5, stride=1)\n",
    "            self.conv3 = L.Convolution2D(\n",
    "                in_channels=16, out_channels=120, ksize=4, stride=1)\n",
    "            self.fc4 = L.Linear(None, 84)\n",
    "            self.fc5 = L.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.sigmoid(self.conv1(x))\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = F.sigmoid(self.conv2(h))\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        h = F.sigmoid(self.conv3(h))\n",
    "        h = F.sigmoid(self.fc4(h))\n",
    "        if chainer.config.train:\n",
    "            return self.fc5(h)\n",
    "        return F.softmax(self.fc5(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cifar10(    \n",
    "    batchsize = 100,\n",
    "    epoch = 50,\n",
    "    gpu0 = -1,\n",
    "    gpu1 = None,\n",
    "    out = 'result',\n",
    "    resume = '',\n",
    "    ):\n",
    "    \n",
    "    print('GPU: {}, {}'.format(gpu0, gpu1))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('')\n",
    "\n",
    "    model = L.Classifier(LeNet5())\n",
    "    if gpu0 >= 0:\n",
    "        chainer.backends.cuda.get_device_from_id(gpu0).use()\n",
    "        if gpu1 is None:\n",
    "            model.to_gpu()  \n",
    "\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    train, test = chainer.datasets.get_cifar10()\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "    if gpu1 is None:\n",
    "        updater = training.updaters.StandardUpdater(\n",
    "            train_iter, optimizer, device=gpu0)\n",
    "    if gpu1 is not None:\n",
    "        updater = training.updaters.ParallelUpdater(\n",
    "            train_iter,\n",
    "            optimizer,\n",
    "            devices={'main': gpu0, 'second': gpu1},\n",
    "        )\n",
    "\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu0),trigger=(epoch, 'epoch'))\n",
    "\n",
    "    trainer.extend(extensions.LogReport(),trigger=(epoch, 'epoch'))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),trigger=(epoch, 'epoch'))\n",
    "    print('\\n')\n",
    "    # Run the training\n",
    "    trainer.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1, None\n",
      "# Minibatch-size: 100\n",
      "# epoch: 50\n",
      "\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          1.00791     1.92314               0.64           0.6087                    1313.48       \n",
      "GPU: 0, None\n",
      "# Minibatch-size: 100\n",
      "# epoch: 50\n",
      "\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          1.26838     1.9345                0.62           0.6039                    127.483       \n",
      "GPU: 0, 1\n",
      "# Minibatch-size: 100\n",
      "# epoch: 50\n",
      "\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          1.02486     1.94677               0.6            0.5919                    273.38        \n"
     ]
    }
   ],
   "source": [
    "run_cifar10()\n",
    "run_cifar10(gpu0=0)\n",
    "run_cifar10(gpu0=0, gpu1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet152(chainer.Chain):\n",
    "    def __init__(self, n_blocks=[3, 8, 36, 3]):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(ResNet152, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 64, 7, 2, 3, initialW=w, nobias=True)\n",
    "            self.bn1 = L.BatchNormalization(64)\n",
    "            self.res2 = ResBlock(n_blocks[0], 64, 64, 256, 1)\n",
    "            self.res3 = ResBlock(n_blocks[1], 256, 128, 512)\n",
    "            self.res4 = ResBlock(n_blocks[2], 512, 256, 1024)\n",
    "            self.res5 = ResBlock(n_blocks[3], 1024, 512, 2048)\n",
    "            self.fc6 = L.Linear(2048, 1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.bn1(self.conv1(x))\n",
    "        h = F.max_pooling_2d(F.relu(h), 2, 2)\n",
    "        h = self.res2(h)\n",
    "        h = self.res3(h)\n",
    "        h = self.res4(h)\n",
    "        h = self.res5(h)\n",
    "        h = F.average_pooling_2d(h, h.shape[2:], stride=1)\n",
    "        h = self.fc6(h)\n",
    "        if chainer.config.train:\n",
    "            return h\n",
    "        return F.softmax(h)\n",
    "\n",
    "\n",
    "class ResBlock(chainer.ChainList):\n",
    "    def __init__(self, n_layers, n_in, n_mid, n_out, stride=2):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.add_link(BottleNeck(n_in, n_mid, n_out, stride, True))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.add_link(BottleNeck(n_out, n_mid, n_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleNeck(chainer.Chain):\n",
    "    def __init__(self, n_in, n_mid, n_out, stride=1, proj=False):\n",
    "        w = chainer.initializers.HeNormal()\n",
    "        super(BottleNeck, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1x1a = L.Convolution2D(\n",
    "                n_in, n_mid, 1, stride, 0, initialW=w, nobias=True)\n",
    "            self.conv3x3b = L.Convolution2D(\n",
    "                n_mid, n_mid, 3, 1, 1, initialW=w, nobias=True)\n",
    "            self.conv1x1c = L.Convolution2D(\n",
    "                n_mid, n_out, 1, 1, 0, initialW=w, nobias=True)\n",
    "            self.bn_a = L.BatchNormalization(n_mid)\n",
    "            self.bn_b = L.BatchNormalization(n_mid)\n",
    "            self.bn_c = L.BatchNormalization(n_out)\n",
    "            if proj:\n",
    "                self.conv1x1r = L.Convolution2D(\n",
    "                    n_in, n_out, 1, stride, 0, initialW=w, nobias=True)\n",
    "                self.bn_r = L.BatchNormalization(n_out)\n",
    "        self.proj = proj\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.bn_a(self.conv1x1a(x)))\n",
    "        h = F.relu(self.bn_b(self.conv3x3b(h)))\n",
    "        h = self.bn_c(self.conv1x1c(h))\n",
    "        if self.proj:\n",
    "            x = self.bn_r(self.conv1x1r(x))\n",
    "        return F.relu(h + x)\n",
    "\n",
    "def run_resnet(    \n",
    "    batchsize = 2048,\n",
    "    epoch = 50,\n",
    "    gpu0 = -1,\n",
    "    gpu1 = None,\n",
    "    out = 'result',\n",
    "    resume = '',\n",
    "    ):\n",
    "    \n",
    "    print('GPU: {}, {}'.format(gpu0, gpu1))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('')\n",
    "\n",
    "    model = L.Classifier(ResNet152())\n",
    "    if gpu0 >= 0:\n",
    "        chainer.backends.cuda.get_device_from_id(gpu0).use()\n",
    "        if gpu1 is None:\n",
    "            model.to_gpu()  \n",
    "\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    train, test = chainer.datasets.get_cifar10()\n",
    "\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
    "                                                 repeat=False, shuffle=False)\n",
    "    if gpu1 is None:\n",
    "        updater = training.updaters.StandardUpdater(\n",
    "            train_iter, optimizer, device=gpu0)\n",
    "    if gpu1 is not None:\n",
    "        updater = training.updaters.ParallelUpdater(\n",
    "            train_iter,\n",
    "            optimizer,\n",
    "            devices={'main': gpu0, 'second': gpu1},\n",
    "        )\n",
    "\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu0),trigger=(epoch, 'epoch'))\n",
    "\n",
    "    trainer.extend(extensions.LogReport(),trigger=(epoch, 'epoch'))\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']),trigger=(epoch, 'epoch'))\n",
    "    print('\\n')\n",
    "    # Run the training\n",
    "    trainer.run()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 0, None\n",
      "# Minibatch-size: 2048\n",
      "# epoch: 50\n",
      "\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          0.0734697   6.44621               0.975586       0.474124                  1211.89       \n",
      "GPU: 0, 1\n",
      "# Minibatch-size: 2048\n",
      "# epoch: 50\n",
      "\n",
      "\n",
      "\n",
      "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy  elapsed_time\n",
      "\u001b[J50          0.0985431   6.45934               0.962891       0.459237                  789.439       \n"
     ]
    }
   ],
   "source": [
    "run_resnet(gpu0=0)\n",
    "run_resnet(gpu0=0, gpu1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Chain):\n",
    "    n_input  = 1\n",
    "    n_output = 1\n",
    "    n_units  = 5\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__(\n",
    "            l1 = L.Linear(self.n_input, self.n_units),\n",
    "            l2 = L.LSTM(self.n_units, self.n_units),\n",
    "            l3 = L.Linear(self.n_units, self.n_output),\n",
    "        )\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.l2.reset_state()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = self.l1(x)\n",
    "        h2 = self.l2(h1)\n",
    "        return self.l3(h2)\n",
    "\n",
    "class LossFuncL(Chain):\n",
    "    def __init__(self, predictor):\n",
    "        super(LossFuncL, self).__init__(predictor=predictor)\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        if (type(x.data)==memoryview):\n",
    "            x = x.reshape((-1, 1)).astype(np.float32)\n",
    "            t = t.reshape((-1, 1)).astype(np.float32)\n",
    "        \n",
    "        y = self.predictor(x)\n",
    "        loss = F.mean_squared_error(y, t)\n",
    "        report({'loss':loss}, self)\n",
    "        return loss\n",
    "\n",
    "class LSTM_test_Iterator(chainer.dataset.Iterator):\n",
    "    def __init__(self, dataset, batch_size = 10, seq_len = 5, repeat = True):\n",
    "        self.seq_length = seq_len\n",
    "        self.dataset = dataset\n",
    "        self.nsamples =  len(dataset)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.repeat = repeat\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.iteration = 0\n",
    "        self.offsets = np.random.randint(0, len(dataset),size=batch_size)\n",
    "\n",
    "        self.is_new_epoch = False\n",
    "\n",
    "    def __next__(self):\n",
    "        if not self.repeat and self.iteration * self.batch_size >= self.nsamples:\n",
    "            raise StopIteration\n",
    "\n",
    "        x, t = self.get_data()\n",
    "        self.iteration += 1\n",
    "\n",
    "        epoch = int(self.epoch_detail)\n",
    "        self.is_new_epoch = self.epoch < epoch\n",
    "        if self.is_new_epoch:\n",
    "            self.epoch = epoch\n",
    "            self.offsets = np.random.randint(0, self.nsamples,size=self.batch_size)\n",
    "\n",
    "        return list(zip(x, t))\n",
    "\n",
    "    @property\n",
    "    def epoch_detail(self):\n",
    "        return self.iteration * self.batch_size / len(self.dataset)\n",
    "\n",
    "    def get_data(self):\n",
    "        tmp0 = [self.dataset[(offset + self.iteration)%self.nsamples][0]\n",
    "               for offset in self.offsets]\n",
    "        tmp1 = [self.dataset[(offset + self.iteration + 1)%self.nsamples][0]\n",
    "               for offset in self.offsets]\n",
    "        return tmp0,tmp1\n",
    "\n",
    "    def serialzie(self, serialzier):\n",
    "        self.iteration = serializer('iteration', self.iteration)\n",
    "        self.epoch     = serializer('epoch', self.epoch)\n",
    "\n",
    "class LSTM_std_updater(training.StandardUpdater):\n",
    "    def __init__(self, train_iter, optimizer, device):\n",
    "        super(LSTM_updater, self).__init__(train_iter, optimizer, device=device)\n",
    "        self.seq_length = train_iter.seq_length\n",
    "\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        for i in range(self.seq_length):\n",
    "            batch = np.array(train_iter.__next__()).astype(np.float32)\n",
    "            x, t  = batch[:,0].reshape((-1,1)), batch[:,1].reshape((-1,1))\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.zerograds()\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        optimizer.update()\n",
    "        \n",
    "class LSTM_prl_updater(training.ParallelUpdater):\n",
    "    def __init__(self, train_iter, optimizer, devices):\n",
    "        super(LSTM_updater, self).__init__(train_iter, optimizer, devices=devices)\n",
    "        self.seq_length = train_iter.seq_length\n",
    "\n",
    "    def update_core(self):\n",
    "        loss = 0\n",
    "\n",
    "        train_iter = self.get_iterator('main')\n",
    "        optimizer = self.get_optimizer('main')\n",
    "\n",
    "        for i in range(self.seq_length):\n",
    "            batch = np.array(train_iter.__next__()).astype(np.float32)\n",
    "            x, t  = batch[:,0].reshape((-1,1)), batch[:,1].reshape((-1,1))\n",
    "            loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
    "\n",
    "        optimizer.target.zerograds()\n",
    "        loss.backward()\n",
    "        loss.unchain_backward()\n",
    "        optimizer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LossFuncL(LSTM())\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "def run_sine(    \n",
    "    batchsize = 10,\n",
    "    epoch = 50,\n",
    "    gpu0 = -1,\n",
    "    gpu1 = None,\n",
    "    out = 'result',\n",
    "    resume = '',\n",
    "    ):\n",
    "    \n",
    "    print('GPU: {}, {}'.format(gpu0, gpu1))\n",
    "    print('# Minibatch-size: {}'.format(batchsize))\n",
    "    print('# epoch: {}'.format(epoch))\n",
    "    print('')\n",
    "\n",
    "    # データ作成\n",
    "    N_data  = 100\n",
    "    N_Loop  = 3\n",
    "    t = np.linspace(0., 2*np.pi*N_Loop, num=N_data)\n",
    "\n",
    "    X = 0.8*np.sin(2.0*t)\n",
    "\n",
    "    # データセット\n",
    "    N_train = int(N_data*0.8)\n",
    "    N_test  = int(N_data*0.2)\n",
    "\n",
    "    tmp_DataSet_X= np.array(X).astype(np.float32)\n",
    "\n",
    "    x_train, x_test = np.array(tmp_DataSet_X[:N_train]),np.array(tmp_DataSet_X[N_train:])\n",
    "\n",
    "    train = tuple_dataset.TupleDataset(x_train)\n",
    "    test  = tuple_dataset.TupleDataset(x_test)\n",
    "\n",
    "    train_iter = LSTM_test_Iterator(train, batch_size = batchsize, seq_len = 10)\n",
    "    test_iter  = LSTM_test_Iterator(test,  batch_size = batchsize, seq_len = 10, repeat = False)\n",
    "\n",
    "\n",
    "    if gpu1 is None:\n",
    "        updater = LSTM_std_updater(train_iter, optimizer, gpu0)\n",
    "    if gpu1 is not None:\n",
    "        updater = LSTM_prl_updater(train_iter, optimizer, devices={'main': gpu0, 'second': gpu1})\n",
    "\n",
    "    \n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out = 'result')\n",
    "\n",
    "    eval_model = model.copy()\n",
    "    eval_rnn = eval_model.predictor\n",
    "    eval_rnn.train = False\n",
    "    trainer.extend(extensions.Evaluator(\n",
    "            test_iter, eval_model, device=gpu0))\n",
    "\n",
    "    trainer.extend(extensions.LogReport())\n",
    "\n",
    "    trainer.extend(\n",
    "            extensions.PlotReport(\n",
    "            ['main/loss', 'validation/main/loss'],\n",
    "            'epoch', file_name='loss.png', trigger=(100, 'epoch')\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "    trainer.extend(\n",
    "            extensions.PrintReport(\n",
    "            ['epoch', 'main/loss', 'validation/main/loss']\n",
    "                )\n",
    "            )\n",
    "\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    trainer.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: 1, None\n",
      "# Minibatch-size: 10\n",
      "# epoch: 50\n",
      "\n",
      "\u001b[J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: incompatible array types are mixed in the forward input (LinearFunction).\n",
      "Actual: <class 'numpy.ndarray'>, <class 'cupy.core.core.ndarray'>, <class 'cupy.core.core.ndarray'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/training/trainer.py\", line 316, in run\n",
      "    update()\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\", line 170, in update\n",
      "    self.update_core()\n",
      "  File \"<ipython-input-86-834bde426f08>\", line 94, in update_core\n",
      "    loss += optimizer.target(chainer.Variable(x), chainer.Variable(t))\n",
      "  File \"<ipython-input-86-834bde426f08>\", line 30, in __call__\n",
      "    y = self.predictor(x)\n",
      "  File \"<ipython-input-86-834bde426f08>\", line 17, in __call__\n",
      "    h1 = self.l1(x)\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/link.py\", line 285, in __call__\n",
      "    out = forward(*args, **kwargs)\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/links/connection/linear.py\", line 144, in forward\n",
      "    return linear.linear(x, self.W, self.b, n_batch_axes=n_batch_axes)\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/functions/connection/linear.py\", line 305, in linear\n",
      "    y, = LinearFunction().apply(args)\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/function_node.py\", line 289, in apply\n",
      "    utils._check_arrays_forward_compatible(in_data, self.label)\n",
      "  File \"/home/member/anaconda3/lib/python3.6/site-packages/chainer/utils/__init__.py\", line 88, in _check_arrays_forward_compatible\n",
      "    ', '.join(str(type(a)) for a in arrays)))\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "incompatible array types are mixed in the forward input (LinearFunction).\nActual: <class 'numpy.ndarray'>, <class 'cupy.core.core.ndarray'>, <class 'cupy.core.core.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-bb223e7767f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_sine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-4ac5ff4f21f7>\u001b[0m in \u001b[0;36mrun_sine\u001b[0;34m(batchsize, epoch, gpu0, gpu1, out, resume)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    347\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Traceback (most recent call last):\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                         \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_tb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, show_loop_exception_msg)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                     \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/training/updaters/standard_updater.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \"\"\"\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-834bde426f08>\u001b[0m in \u001b[0;36mupdate_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzerograds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-834bde426f08>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-834bde426f08>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/link.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# forward is implemented in the child classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mforward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# Call forward_postprocess hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/links/connection/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, n_batch_axes)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_of_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batch_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, W, b, n_batch_axes)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_batch_axes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/function_node.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchainerx_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainerx_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_arrays_forward_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mis_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/chainer/utils/__init__.py\u001b[0m in \u001b[0;36m_check_arrays_forward_compatible\u001b[0;34m(arrays, label)\u001b[0m\n\u001b[1;32m     86\u001b[0m             'Actual: {}'.format(\n\u001b[1;32m     87\u001b[0m                 \u001b[0;34m' ({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 ', '.join(str(type(a)) for a in arrays)))\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: incompatible array types are mixed in the forward input (LinearFunction).\nActual: <class 'numpy.ndarray'>, <class 'cupy.core.core.ndarray'>, <class 'cupy.core.core.ndarray'>"
     ]
    }
   ],
   "source": [
    "run_sine(gpu0=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
